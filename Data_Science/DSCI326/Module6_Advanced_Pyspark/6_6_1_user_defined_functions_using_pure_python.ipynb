{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions\n",
    "\n",
    "Recall that the`pandas` `Series.apply` and `Series.map` methods allowed us to apply functions to each individual data value in a column.  We would like to do the same in `pyspark`, but the underlying code runs in Scala, we need some extra machinery to apply/map a function to the data. Our two solitions are\n",
    "\n",
    "- Define a Python `udf` column function that will run Python code on each data value.\n",
    "- Define a `pandas_udf` that is faster and uses existing pandas methods to perform [vectorized operations](https://en.wikipedia.org/wiki/Array_programming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined Python functions\n",
    "\n",
    "In this notebook, we will start by looking at defining pure Python user defined functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sets\n",
    "\n",
    "We will be using two of the data sets provided by the Museam of Modern Art (MoMA) in this lecture.  Make sure that you have downloaded each repository.  [Download Instructions](./get_MOMA_data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/03 10:34:41 WARN Utils: Your hostname, nn1448lr222 resolves to a loopback address: 127.0.1.1; using 172.22.172.10 instead (on interface eth0)\n",
      "22/11/03 10:34:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/03 10:34:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/11/03 10:34:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from more_pyspark import get_spark_types, to_pandas\n",
    "\n",
    "spark = SparkSession.builder.appName('Ops').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MoMA_schema import artwork_schema\n",
    "\n",
    "artists = spark.read.csv(\"./data/Artists.csv\", inferSchema=True, header=True)\n",
    "artwork = spark.read.csv(\"./data/Artworks.csv\", schema=artwork_schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ConstituentID</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>ArtistBio</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Gender</th>\n",
       "      <th>BeginDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Wiki QID</th>\n",
       "      <th>ULAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Robert Arneson</td>\n",
       "      <td>American, 1930–1992</td>\n",
       "      <td>American</td>\n",
       "      <td>Male</td>\n",
       "      <td>1930</td>\n",
       "      <td>1992</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Doroteo Arnaiz</td>\n",
       "      <td>Spanish, born 1936</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Male</td>\n",
       "      <td>1936</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ConstituentID     DisplayName            ArtistBio Nationality Gender  \\\n",
       "0              1  Robert Arneson  American, 1930–1992    American   Male   \n",
       "1              2  Doroteo Arnaiz   Spanish, born 1936     Spanish   Male   \n",
       "\n",
       "   BeginDate  EndDate Wiki QID  ULAN  \n",
       "0       1930     1992     None  None  \n",
       "1       1936        0     None  None  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists.take(2) >> to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ConstituentID: integer (nullable = true)\n",
      " |-- DisplayName: string (nullable = true)\n",
      " |-- ArtistBio: string (nullable = true)\n",
      " |-- Nationality: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- BeginDate: integer (nullable = true)\n",
      " |-- EndDate: integer (nullable = true)\n",
      " |-- Wiki QID: string (nullable = true)\n",
      " |-- ULAN: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/03 10:35:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Artist</th>\n",
       "      <th>ConstituentID</th>\n",
       "      <th>ArtistBio</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>BeginDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Date</th>\n",
       "      <th>Medium</th>\n",
       "      <th>...</th>\n",
       "      <th>ThumbnailURL</th>\n",
       "      <th>Circumference (cm)</th>\n",
       "      <th>Depth (cm)</th>\n",
       "      <th>Diameter (cm)</th>\n",
       "      <th>Height (cm)</th>\n",
       "      <th>Length (cm)</th>\n",
       "      <th>Weight (kg)</th>\n",
       "      <th>Width (cm)</th>\n",
       "      <th>Seat Height (cm)</th>\n",
       "      <th>Duration (sec.)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ferdinandsbrücke Project, Vienna, Austria (Ele...</td>\n",
       "      <td>Otto Wagner</td>\n",
       "      <td>6210</td>\n",
       "      <td>(Austrian, 1841–1918)</td>\n",
       "      <td>(Austrian)</td>\n",
       "      <td>(1841)</td>\n",
       "      <td>(1918)</td>\n",
       "      <td>(Male)</td>\n",
       "      <td>1896</td>\n",
       "      <td>Ink and cut-and-pasted painted pages on paper</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.moma.org/media/W1siZiIsIjU5NDA1Il0s...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>48.6000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>168.9000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>City of Music, National Superior Conservatory ...</td>\n",
       "      <td>Christian de Portzamparc</td>\n",
       "      <td>7470</td>\n",
       "      <td>(French, born 1944)</td>\n",
       "      <td>(French)</td>\n",
       "      <td>(1944)</td>\n",
       "      <td>(0)</td>\n",
       "      <td>(Male)</td>\n",
       "      <td>1987</td>\n",
       "      <td>Paint and colored pencil on print</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.moma.org/media/W1siZiIsIjk3Il0sWyJw...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>40.6401</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>29.8451</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Ferdinandsbrücke Project, Vienna, Austria (Ele...   \n",
       "1  City of Music, National Superior Conservatory ...   \n",
       "\n",
       "                     Artist ConstituentID              ArtistBio Nationality  \\\n",
       "0               Otto Wagner          6210  (Austrian, 1841–1918)  (Austrian)   \n",
       "1  Christian de Portzamparc          7470    (French, born 1944)    (French)   \n",
       "\n",
       "  BeginDate EndDate  Gender  Date  \\\n",
       "0    (1841)  (1918)  (Male)  1896   \n",
       "1    (1944)     (0)  (Male)  1987   \n",
       "\n",
       "                                          Medium  ...  \\\n",
       "0  Ink and cut-and-pasted painted pages on paper  ...   \n",
       "1              Paint and colored pencil on print  ...   \n",
       "\n",
       "                                        ThumbnailURL Circumference (cm)  \\\n",
       "0  http://www.moma.org/media/W1siZiIsIjU5NDA1Il0s...               None   \n",
       "1  http://www.moma.org/media/W1siZiIsIjk3Il0sWyJw...               None   \n",
       "\n",
       "  Depth (cm) Diameter (cm) Height (cm) Length (cm) Weight (kg) Width (cm)  \\\n",
       "0       None          None     48.6000        None        None   168.9000   \n",
       "1       None          None     40.6401        None        None    29.8451   \n",
       "\n",
       "  Seat Height (cm) Duration (sec.)  \n",
       "0             None            None  \n",
       "1             None            None  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artwork.take(2) >> to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Artist: string (nullable = true)\n",
      " |-- ConstituentID: string (nullable = true)\n",
      " |-- ArtistBio: string (nullable = true)\n",
      " |-- Nationality: string (nullable = true)\n",
      " |-- BeginDate: string (nullable = true)\n",
      " |-- EndDate: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Medium: string (nullable = true)\n",
      " |-- Dimensions: string (nullable = true)\n",
      " |-- CreditLine: string (nullable = true)\n",
      " |-- AccessionNumber: string (nullable = true)\n",
      " |-- Classification: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- DateAcquired: string (nullable = true)\n",
      " |-- Cataloged: string (nullable = true)\n",
      " |-- ObjectID: string (nullable = true)\n",
      " |-- URL: string (nullable = true)\n",
      " |-- ThumbnailURL: string (nullable = true)\n",
      " |-- Circumference (cm): double (nullable = true)\n",
      " |-- Depth (cm): double (nullable = true)\n",
      " |-- Diameter (cm): double (nullable = true)\n",
      " |-- Height (cm): double (nullable = true)\n",
      " |-- Length (cm): double (nullable = true)\n",
      " |-- Weight (kg): double (nullable = true)\n",
      " |-- Width (cm): double (nullable = true)\n",
      " |-- Seat Height (cm): double (nullable = true)\n",
      " |-- Duration (sec.): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artwork.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and applying a `udf` in `pyspark`\n",
    "\n",
    "* **udf**: <b>U</b>ser <b>D</b>efined <b>F</b>unction\n",
    "* Use `pyspark.sql.functions.udf(func, pyspark_type)` to define the function\n",
    "* Use the `udf` inside `withColumn` to make/change columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 -  Compute the Century of Birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "century = udf(lambda year: (int(year)//100)*100,\n",
    "              IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'<lambda>(BeginDate)'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "century(artists.BeginDate) # lazy column expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BeginDate</th>\n",
       "      <th>Century of Birth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1930</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1936</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1941</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BeginDate  Century of Birth\n",
       "0       1930              1900\n",
       "1       1936              1900\n",
       "2       1941              1900"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(artists\n",
    ".select('BeginDate')\n",
    ".withColumn('Century of Birth', century(artists.BeginDate))\n",
    ".take(3)\n",
    ") >> to_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 - Indicator column using a Conditional expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nationality</th>\n",
       "      <th>American</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>American</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Nationality  American\n",
       "0    American         1\n",
       "1     Spanish         0\n",
       "2    American         1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_american = udf(lambda n: 1 if n == 'American' else 0,\n",
    "                 IntegerType())\n",
    "\n",
    "(artists\n",
    ".select(artists.Nationality)\n",
    ".withColumn('American', is_american(artists.Nationality))\n",
    ".take(3)\n",
    ") >> to_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 - Applying the $\\log + 1$ transformation \n",
    "\n",
    "In statistics, it is common to apply a $\\log$ transformation to correct right skew.  Since $log(0)=-\\infty$, it is customary to add one before applying the $\\log$ to data that might have zeros.  Together the $y=\\log(x+1)$ is known as the \"log plus 1\" transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, e\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "lnp1 = lambda num: log(num + 1, e)\n",
    "\n",
    "lnp1_spark = udf(lnp1, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/03 10:38:03 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 6)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1327/3049692649.py\", line 4, in <lambda>\n",
      "TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/03 10:38:03 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 6) (172.22.172.10 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1327/3049692649.py\", line 4, in <lambda>\n",
      "TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/11/03 10:38:03 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_1327/3049692649.py\", line 4, in <lambda>\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (\u001b[43martwork\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHeight (cm)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mln(Height + 1)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlnp1_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHeight (cm)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/pyspark/sql/dataframe.py:868\u001b[0m, in \u001b[0;36mDataFrame.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;124;03m    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_1327/3049692649.py\", line 4, in <lambda>\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n"
     ]
    }
   ],
   "source": [
    "(artwork\n",
    " .select('Height (cm)')\n",
    " .withColumn('ln(Height + 1)', lnp1_spark(col('Height (cm)')))\n",
    " .take(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What went wrong?\n",
    "\n",
    "The `unsupported operand type(s) for +: 'NoneType' and 'int'` means that we tried to add `None` and `1`, which is undefined.  This problem is so common that the solution has it's own name--the [null object pattern](https://en.wikipedia.org/wiki/Null_object_pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlnp1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(num)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m log, e\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DoubleType\n\u001b[0;32m----> 4\u001b[0m lnp1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m num: log(\u001b[43mnum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m, e)\n\u001b[1;32m      6\u001b[0m lnp1_spark \u001b[38;5;241m=\u001b[39m udf(lnp1, DoubleType())\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "lnp1(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's the solution?\n",
    "\n",
    "Check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnp1 = lambda num: None if num is None else log(num + 1, e)\n",
    "\n",
    "lnp1_spark_V2 = udf(lnp1, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height (cm)</th>\n",
       "      <th>ln(Height + 1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.6000</td>\n",
       "      <td>3.903991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.6401</td>\n",
       "      <td>3.729064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.3000</td>\n",
       "      <td>3.563883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Height (cm)  ln(Height + 1)\n",
       "0      48.6000        3.903991\n",
       "1      40.6401        3.729064\n",
       "2      34.3000        3.563883"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(artwork\n",
    " .select('Height (cm)')\n",
    " .withColumn('ln(Height + 1)', lnp1_spark_V2(col('Height (cm)')))\n",
    " .take(3)\n",
    ") >> to_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's a fancier solution?\n",
    "\n",
    "Think about the type of our most recent function.  We know that a `float` becomes a `float`, but `None` remains `None`.  This type is known as the [Maybe monad](https://en.wikipedia.org/wiki/Monad_(functional_programming)#An_example:_Maybe), a type that you will reinvent 9,000 times in your career.  \n",
    "\n",
    "Let's create a more gneral solution by creating a wrapper function that will decorate another function, handling the null object issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_apply(func):\n",
    "    ''' Decorates a function to account for missing input (e.g. None).\n",
    "    \n",
    "    args:\n",
    "        func - A unary function\n",
    "        \n",
    "    returns:\n",
    "        a unary function that accepts an input (say x) returns \n",
    "            - None when x is None\n",
    "            - func(x) otherwise\n",
    "    '''\n",
    "    return lambda x: None if x is None else func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height (cm)</th>\n",
       "      <th>log (height + 1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.6000</td>\n",
       "      <td>3.903991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.6401</td>\n",
       "      <td>3.729064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Height (cm)  log (height + 1)\n",
       "0      48.6000          3.903991\n",
       "1      40.6401          3.729064"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "ln1p = lambda num: log(num + 1, e)\n",
    "\n",
    "spark_log1p_V3 = udf(maybe_apply(ln1p), DoubleType())\n",
    "\n",
    "(artwork\n",
    " .select('Height (cm)')\n",
    " .withColumn('log (height + 1)', spark_log1p_V3(col('Height (cm)')))\n",
    " .take(2)\n",
    ") >> to_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Exercise 6.6.1 </font>\n",
    "\n",
    "Solve each of the following tasks by creating and applying a Python `udf`.\n",
    "\n",
    "1. Use the string `replace` method to remove parentheses from the `artwork.Nationality` column.\n",
    "2. Convert the weight column to pounds.\n",
    "\n",
    "In both cases, you should check for missing values and proceed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Austrian'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hint 1 - the string replace method doesn't accept regular expr and must be chained\n",
    "\n",
    "'(Austrian)'.replace('(', '').replace(')', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Austrian'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hint 2 - use re.sub if you want to use regex\n",
    "from re import sub\n",
    "\n",
    "sub('[()]', '', '(Austrian)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austrian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Nationality\n",
       "0    Austrian\n",
       "1      French"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/03 11:05:43 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 283808 ms exceeds timeout 120000 ms\n",
      "22/11/03 11:05:43 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from re import sub\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "replace_r = lambda string: sub('[()]','',string)\n",
    "\n",
    "replace = udf(maybe_apply(replace_r), StringType())\n",
    "\n",
    "(artwork\n",
    " .select('Nationality')\n",
    " .withColumn('Nationality', replace(col('Nationality')))\n",
    " .take(2)\n",
    ") >> to_pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
