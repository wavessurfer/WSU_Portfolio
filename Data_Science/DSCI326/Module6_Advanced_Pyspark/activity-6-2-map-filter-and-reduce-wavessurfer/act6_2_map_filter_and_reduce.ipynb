{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 11:16:10 WARN Utils: Your hostname, nn1448lr222 resolves to a loopback address: 127.0.1.1; using 172.22.171.23 instead (on interface eth0)\n",
      "22/11/01 11:16:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 11:16:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/11/01 11:16:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"count app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Time between earthquakes\n",
    "\n",
    "Suppose that earthquakes of a certain magnitude in a specific region can be modeled as a Poisson process with a mean of $\\lambda = 4.5$ earthquakes per day.  Let $X$ be the time until the third earth quake.  It can be shown that $X$ has a $Gamma$ distribution with $\\alpha = 3$ (number of events) and $\\beta = \\frac{1}{\\lambda}=\\frac{1}{4.5}$ (average time until the 3rd earthquake).  We can use Python's `random.gammavariate` to simulate the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from random import gammavariate\n",
    "?gammavariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.926665593480046,\n",
       " 0.7737326767690611,\n",
       " 0.7445581120773845,\n",
       " 0.6615235213640299,\n",
       " 0.8333565811335074]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from composable.sequence import head\n",
    "N = 1000000\n",
    "time_between_3_quakes = [gammavariate(3,1/4.5) for i in range(N)]\n",
    "time_between_3_quakes >> head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three `for` loop patterns\n",
    "\n",
    "Most all `for` loops are reinventing one of the following patterns.\n",
    "\n",
    "1. **Map**ping a function/transformation unto each value.\n",
    "2. **Filter**ing the values by some boolean condition.\n",
    "3. **Reduce** values to one or more statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map example - Convert the times from days to hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22.239974243521104,\n",
       " 18.569584242457466,\n",
       " 17.86939468985723,\n",
       " 15.876564512736717,\n",
       " 20.00055794720418]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop solution\n",
    "time_in_hours = []\n",
    "for t in time_between_3_quakes:\n",
    "    time_in_hours.append(t*24)\n",
    "time_in_hours >> head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22.239974243521104,\n",
       " 18.569584242457466,\n",
       " 17.86939468985723,\n",
       " 15.876564512736717,\n",
       " 20.00055794720418]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprehension solution\n",
    "time_in_hours = [t*24 for t in time_between_3_quakes]\n",
    "time_in_hours >> head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22.239974243521104,\n",
       " 18.569584242457466,\n",
       " 17.86939468985723,\n",
       " 15.876564512736717,\n",
       " 20.00055794720418]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With pipeable functions\n",
    "from composable.strict import map\n",
    "\n",
    "(time_between_3_quakes\n",
    " >> map(lambda t: t*24)\n",
    " >> head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Example -  filter out all value less than 1 day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.926665593480046,\n",
       " 0.7737326767690611,\n",
       " 0.7445581120773845,\n",
       " 0.6615235213640299,\n",
       " 0.8333565811335074]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop solution\n",
    "less_than_1_day = []\n",
    "for t in time_between_3_quakes:\n",
    "    if t < 1:\n",
    "        less_than_1_day.append(t)\n",
    "less_than_1_day >> head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.926665593480046,\n",
       " 0.7737326767690611,\n",
       " 0.7445581120773845,\n",
       " 0.6615235213640299,\n",
       " 0.8333565811335074]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comprehension solution\n",
    "less_than_1_day = [t for t in time_between_3_quakes if t < 1]\n",
    "less_than_1_day >> head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.926665593480046,\n",
       " 0.7737326767690611,\n",
       " 0.7445581120773845,\n",
       " 0.6615235213640299,\n",
       " 0.8333565811335074]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeable functions\n",
    "from composable.strict import filter\n",
    "\n",
    "(time_between_3_quakes\n",
    " >> filter(lambda t: t < 1)\n",
    " >> head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reduce Example - Accumulating the maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.629679952322248"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loop solution\n",
    "max_time = 0 # safe since Gamma is non-negative\n",
    "for t in time_between_3_quakes:\n",
    "    max_time = max(max_time, t) # update step\n",
    "max_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.629679952322248"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functional solution\n",
    "from functools import reduce\n",
    "\n",
    "reduce(lambda m, t: max(m, t), time_between_3_quakes, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeable solution\n",
    "from composable import pipeable\n",
    "\n",
    "update_max = lambda m, t: max(m, t)\n",
    "\n",
    "@pipeable\n",
    "def my_reduce(func, xs, init = None):\n",
    "    if init is None:\n",
    "        return reduce(func, xs) # Uses first value as init\n",
    "    else:\n",
    "        return reduce(func, xs, init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.629679952322248"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with init = 0\n",
    "(time_between_3_quakes\n",
    " >> my_reduce(update_max, init = 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.629679952322248"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with init = first value\n",
    "(time_between_3_quakes\n",
    " >> my_reduce(update_max)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So Iverson, why are you such a piping fanboi!?!??\n",
    "\n",
    "Legos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "952"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 11:49:13 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 282440 ms exceeds timeout 120000 ms\n",
      "22/11/01 11:49:14 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Find the number of time less than 1 hour\n",
    "(time_between_3_quakes\n",
    " >> map(lambda t: 24*t)\n",
    " >> filter(lambda t: t < 1)\n",
    " >> my_reduce(lambda cnt, t: cnt + 1, init = 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> Task 6.1.3 </font>\n",
    "\n",
    "Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can stack them up and keep building something bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So ... about those loops ...\n",
    "\n",
    "<img src=\"./img/no_more_for_loops.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops don't work well on multiple or multi-core machines\n",
    "\n",
    "<img src=\"./img/loop_problems.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about functions?\n",
    "\n",
    "* Using [lambda calculus](https://en.wikipedia.org/wiki/Lambda_calculus) we can show that all functional programs that terminate will provide the same result regardless of the order of execution.\n",
    "* This explains why `pyspark` uses functional idioms like `map`, `filter`, and `reduce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The `pyspark.RDD` \n",
    "\n",
    "> A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned  collection of elements that can be operated on in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_RDD = sc.parallelize(time_between_3_quakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 11:56:50 WARN TaskSetManager: Stage 1 contains a task of very large size (8820 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "952"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of time less than 1 hour\n",
    "(times_RDD\n",
    " .map(lambda t: 24*t)\n",
    " .filter(lambda t: t < 1)\n",
    " .map(lambda t: 1)\n",
    " .cache()\n",
    " .reduce(lambda cnt, t: cnt + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color=\"red\"><h2> Task 6.1.2 </h2></font>\n",
    "\n",
    "Use our three functional idioms to compute the average time (in seconds) of all times greater than 1 hour, in two ways.\n",
    "\n",
    "1. In python using the various `pipeable` functions presented earlier.\n",
    "2. Using `pyspark` RDD's\n",
    "\n",
    "**Hint 1.** Computing the mean requires that we (A) compute both the total and count, then (B) divide.\n",
    "\n",
    "**Hint 2.** Allow yourself two passes through the data for 2. and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57649.43885130581"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeable function solution\n",
    "from operator import add\n",
    "\n",
    "filtered_time_seconds  = (time_between_3_quakes\n",
    " >> filter(lambda t: 24*t > 1)\n",
    " >> map(lambda t: t*24*3600)\n",
    ")\n",
    "\n",
    "total = filtered_time_seconds >> my_reduce(add)\n",
    "count = filtered_time_seconds >> my_reduce(lambda x,y: x+1, init = 0)\n",
    "total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 12:15:05 WARN TaskSetManager: Stage 6 contains a task of very large size (8820 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 12:15:06 WARN TaskSetManager: Stage 7 contains a task of very large size (8820 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57649.43885130581"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark RDD solution\n",
    "\n",
    "filtered_Rdd = (times_RDD\n",
    " .filter(lambda t: t*24 > 1)\n",
    " .map(lambda t: t*24*3600)\n",
    ")\n",
    "\n",
    "total_rdd = filtered_Rdd.reduce(add)\n",
    "count_rdd = filtered_Rdd.map(lambda x:1).reduce(add)\n",
    "total_rdd / count_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color=\"red\"><h2> Task 6.1.3 </h2></font>\n",
    "\n",
    "The variance of a random variable is the average square of the difference between $X$ and it's mean.  Use the three functional idioms to compute the variance of the times in three ways:\n",
    "\n",
    "1. In python using a `for` loop.\n",
    "2. In python using the various `pipeable` functions presented earlier.\n",
    "3. Using `pyspark` RDD's\n",
    "\n",
    "**Hint 1.** It can be shown that the mean of our distribution is $\\alpha*\\beta = \\frac{3}{4.5}$.\n",
    "\n",
    "**Hint 2.** Subtract, then square, then average.\n",
    "\n",
    "**Hint 3.** In this case, we can show $V(X) = \\alpha\\beta^2 = \\frac{3}{4.5^2}$.  Use this to check your approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14825316376314696"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import variance\n",
    "\n",
    "variance(time_between_3_quakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14825301661559392"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeable function solution\n",
    "mu = 3/4.5\n",
    "\n",
    "standardized = (time_between_3_quakes\n",
    "               >> map(lambda t: (t-mu)**2)\n",
    "               )\n",
    "\n",
    "total = standardized >> my_reduce(add)\n",
    "count = standardized >> my_reduce(lambda x,y: x+1, init = 0)\n",
    "total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 12:22:44 WARN TaskSetManager: Stage 8 contains a task of very large size (8820 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/11/01 12:22:44 WARN TaskSetManager: Stage 9 contains a task of very large size (8820 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14825301661559392"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark RDD solution\n",
    "standardizedRDD = (times_RDD.map(lambda t: (t-mu)**2)\n",
    "               )\n",
    "\n",
    "total_rdd = standardizedRDD.reduce(add)\n",
    "count_rdd = standardizedRDD.map(lambda x:1).reduce(add)\n",
    "total_rdd / count_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
